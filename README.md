# Instagram Tools - Sistema Integrado

Este proyecto integra el Dashboard Web y el Scraper de Instagram en un sistema unificado usando Docker Compose.

## üèóÔ∏è Arquitectura

```
Instagram Tools/
‚îú‚îÄ‚îÄ dashboard_web/          # Dashboard principal
‚îú‚îÄ‚îÄ scraper/               # Scraper con backend Django y frontend SvelteKit
‚îú‚îÄ‚îÄ docker-compose.yaml    # Configuraci√≥n principal
‚îú‚îÄ‚îÄ nginx.conf            # Proxy reverso
‚îî‚îÄ‚îÄ env.example           # Variables de entorno
```

## üöÄ Servicios

### App Shell (Integradora Principal)
- **Puerto**: 3000
- **URL**: http://localhost:3000
- **Descripci√≥n**: Aplicaci√≥n integradora con navegaci√≥n unificada

### Dashboard Web
- **Puerto**: 5174
- **URL**: http://localhost:5174
- **Descripci√≥n**: Dashboard con visualizaci√≥n de datos

### Scraper Backend (API)
- **Puerto**: 8000
- **URL**: http://localhost:8000
- **Admin**: http://localhost:8000/admin/
- **Descripci√≥n**: API Django para scraping de Instagram

### Scraper Frontend (Opcional)
- **Puerto**: 5173
- **URL**: http://localhost:5173
- **Perfil**: scraper-ui
- **Descripci√≥n**: Interfaz independiente del scraper

### Nginx (Producci√≥n)
- **Puerto**: 80
- **URL**: http://localhost
- **Perfil**: production
- **Descripci√≥n**: Proxy reverso para producci√≥n

## üìã Comandos de Uso

### Configuraci√≥n Inicial
```bash
# Copiar variables de entorno
cp env.example .env

# Editar configuraci√≥n si es necesario
nano .env
```

### Desarrollo (Dashboard + Backend)
```bash
# Iniciar servicios principales
docker compose up -d

# Ver logs
docker compose logs -f

# Ver logs de un servicio espec√≠fico
docker compose logs -f dashboard
docker compose logs -f scraper-backend
```

### Desarrollo Completo (incluye Scraper Frontend)
```bash
# Iniciar todos los servicios incluyendo scraper frontend
docker compose --profile scraper-ui up -d
```

### Producci√≥n (con Nginx)
```bash
# Iniciar con proxy reverso
docker compose --profile production up -d
```

### Gesti√≥n de Contenedores
```bash
# Detener servicios
docker compose down

# Reconstruir im√°genes
docker compose build

# Reconstruir y reiniciar
docker compose up --build -d

# Limpiar vol√∫menes
docker compose down -v
```

## üîó Conexiones

### Dashboard ‚Üí Scraper Backend
- El dashboard se conecta al backend del scraper via `VITE_SCRAPER_API_URL`
- URL interna: `http://scraper-backend:8000`
- URL externa: `http://localhost:8000`

### Scraper Frontend ‚Üí Scraper Backend
- El frontend del scraper se conecta a su propio backend
- URL interna: `http://scraper-backend:8000`
- URL externa: `http://localhost:8000`

## üåê URLs de Acceso

### Desarrollo
- **App Shell (Principal)**: http://localhost:3000
- **Dashboard**: http://localhost:5174
- **Scraper Backend API**: http://localhost:8000
- **Scraper Admin**: http://localhost:8000/admin/
- **Scraper Frontend**: http://localhost:5173 (con perfil scraper-ui)

### Producci√≥n (con Nginx)
- **Dashboard**: http://localhost
- **API**: http://localhost/api/
- **Admin**: http://localhost/admin/
- **Scraper UI**: http://localhost/scraper/

## üìÅ Variables de Entorno

### Dashboard
- `VITE_SCRAPER_API_URL`: URL del backend del scraper

### Scraper Backend
- `DEBUG`: True/False
- `SECRET_KEY`: Clave secreta de Django
- `CORS_ALLOWED_ORIGINS`: Or√≠genes permitidos para CORS
- `DATABASE_URL`: URL de la base de datos
- `TIME_ZONE`: Zona horaria
- `LANGUAGE_CODE`: C√≥digo de idioma

### Scraper Frontend
- `VITE_API_URL`: URL del backend
- `NODE_ENV`: Entorno de Node.js

## üîß Perfiles Disponibles

### `scraper-ui`
Incluye el frontend del scraper:
```bash
docker compose --profile scraper-ui up -d
```

### `production`
Incluye Nginx como proxy reverso:
```bash
docker compose --profile production up -d
```

## üìä Vol√∫menes

- `scraper_backend_data`: Base de datos del scraper
- `scraper_frontend_svelte_kit`: Archivos generados por SvelteKit

## üåê Redes

- `instagram-network`: Red interna para comunicaci√≥n entre servicios

## üîç Troubleshooting

### Si el dashboard no puede conectar con el scraper:
1. Verifica que el scraper-backend est√© corriendo: `docker compose logs scraper-backend`
2. Verifica la variable `VITE_SCRAPER_API_URL` en el dashboard
3. Aseg√∫rate de que CORS est√© configurado correctamente

### Si hay problemas con CORS:
1. Verifica que `CORS_ALLOWED_ORIGINS` incluya las URLs correctas
2. Reinicia el contenedor del backend: `docker compose restart scraper-backend`

### Para reiniciar desde cero:
```bash
docker compose down -v
docker compose build --no-cache
docker compose up -d
```

## üöÄ Pr√≥ximos Pasos

1. **Configurar el dashboard** para usar la API del scraper
2. **Implementar autenticaci√≥n** entre servicios
3. **Configurar SSL** para producci√≥n
4. **Agregar monitoreo** y logs centralizados 